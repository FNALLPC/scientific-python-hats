{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# LIGO Black Holes\n",
    "\n",
    "A real-science demo using Numpy, SciPy, and Pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "LIGO presented their data analysis in an [interactive Jupyter notebook](https://github.com/minrk/ligo-binder) using Numpy, SciPy, Matplotlib, and HDF5. This notebook is a simplification of that analysis, in which I've added Pandas and Vega plotting.\n",
    "\n",
    "Pandas reduced the date-time and plotting code by about 10 lines â†’ 1 line.\n",
    "\n",
    "The signal processing is over my head; I just copied it.\n",
    "\n",
    "We'll need a few more libraries for this one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install h5py pdvega scipy --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install matplotlib==2.0.2 --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Python standard library\n",
    "import json\n",
    "import datetime\n",
    "\n",
    "# scientific Python ecosystem\n",
    "import numpy\n",
    "import pandas\n",
    "import pdvega\n",
    "import scipy.interpolate   # SciPy is a bag of classic algorithms\n",
    "import scipy.signal        # including signal processing\n",
    "import matplotlib.mlab     # some signal processing in Matplotlib (poor separation of concerns...)\n",
    "import h5py\n",
    "\n",
    "# LIGO-specific library for interpreting their HDF5 files; like \"fwlite\"\n",
    "import readligo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "All of the plots can be made with Matplotlib (blurry but fast: it's just PNGs in notebook) or Vega (sharp and interactive: it's Javascript). To use Matplotlib, you need to execute this magic:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "LIGO distributes their data as arrays in HDF5 files accompanied by metadata in JSON file for interpreting them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "events = json.load(open(\"LIGO/BBH_events_v3.json\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The JSON files describe multiple events. Let's take the one from Jan 4, 2017."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "event = events[\"GW170104\"]\n",
    "event"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Load the array data. H1 is a 4 km interferometer at Hanford, Washington, and L1 is a 4 km interferometer at Livingston Parish, Louisiana. The data are indexed by time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strain_H1, time_H1, chan_dict_H1 = readligo.loaddata(\"LIGO/\" + event[\"fn_H1\"], \"H1\")\n",
    "strain_L1, time_L1, chan_dict_L1 = readligo.loaddata(\"LIGO/\" + event[\"fn_L1\"], \"L1\")\n",
    "\n",
    "df_H1 = pandas.DataFrame(index=time_H1, data={\"H1\": strain_H1})\n",
    "df_L1 = pandas.DataFrame(index=time_L1, data={\"L1\": strain_L1})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pandas is for manipulating data with different indexes. If LIGO had used Pandas, they wouldn't have had to ensure that `time_H1 == time_L1`. You can just combine them like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pandas.concat([df_H1, df_L1], axis=1)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The time index is the number of seconds since some date that is not 1970 (the UNIX standard). Fortunately, LIGO provided a Rosetta stone: a single timestamp as text and as a number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "event[\"tevent\"], event[\"utcevent\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Use Pandas tools to turn the index into `datetimes`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "originutc = pandas.to_datetime(event[\"utcevent\"])\n",
    "originsec = pandas.to_datetime(event[\"tevent\"], unit=\"s\")\n",
    "originutc - originsec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_offset = (originutc - originsec).total_seconds()\n",
    "df.index = pandas.to_datetime(df.index + time_offset, unit=\"s\")\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The data are 30 seconds wide, but the signal is only a fraction of a second around the origin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.index.max() - df.index.min()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We make filters (Numpy arrays of booleans), just as in the Numpy notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wide_window = numpy.logical_and(\n",
    "    originutc - datetime.timedelta(seconds=5) <= df.index,\n",
    "    df.index < originutc + datetime.timedelta(seconds=5))\n",
    "\n",
    "narrow_window = numpy.logical_and(\n",
    "    originutc - datetime.timedelta(seconds=0.15) <= df.index,\n",
    "    df.index < originutc + datetime.timedelta(seconds=0.05))\n",
    "\n",
    "wide_window[:len(wide_window) // 2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Now let's plot it in this window. Vega is a little slow (turn off interactivity), but Matplotlib is blurry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[wide_window].vgplot.line(interactive=False)\n",
    "# df[wide_window].plot.line()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The signal is deeply buried in that plot. To bring it out, they use signal processing. I'm not going to pretend to understand exactly what these filters do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Pxx_H1, freqs = matplotlib.mlab.psd(strain_H1, Fs=event[\"fs\"], NFFT=4*event[\"fs\"])\n",
    "Pxx_L1, freqs = matplotlib.mlab.psd(strain_L1, Fs=event[\"fs\"], NFFT=4*event[\"fs\"])\n",
    "\n",
    "psd_H1 = scipy.interpolate.interp1d(freqs, Pxx_H1)\n",
    "psd_L1 = scipy.interpolate.interp1d(freqs, Pxx_L1)\n",
    "\n",
    "dt = time_H1[1] - time_H1[0]   # uniformly sampled\n",
    "\n",
    "def whiten(strain, interp_psd):\n",
    "    Nt = len(strain)\n",
    "    freqs = numpy.fft.rfftfreq(Nt, dt)\n",
    "    freqs1 = numpy.linspace(0, 2048, Nt // 2 + 1)\n",
    "    hf = numpy.fft.rfft(strain)\n",
    "    white_hf = hf / (numpy.sqrt(interp_psd(freqs) / dt / 2.0))\n",
    "    white_ht = numpy.fft.irfft(white_hf, n=Nt)\n",
    "    return white_ht\n",
    "\n",
    "strain_H1_whiten = whiten(strain_H1, psd_H1)\n",
    "strain_L1_whiten = whiten(strain_L1, psd_L1)\n",
    "\n",
    "bb, ab = scipy.signal.butter(\n",
    "    4, [event[\"fband\"][0]*2.0/event[\"fs\"],\n",
    "        event[\"fband\"][1]*2.0/event[\"fs\"]], btype=\"band\")\n",
    "\n",
    "strain_H1_whitenbp = scipy.signal.filtfilt(bb, ab, strain_H1_whiten)\n",
    "strain_L1_whitenbp = scipy.signal.filtfilt(bb, ab, strain_L1_whiten)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Add these new arrays to the data frame (with the same index)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"L1 whitened\"] = pandas.Series(strain_L1_whitenbp, index=df.index)\n",
    "df[\"H1 whitened\"] = pandas.Series(strain_H1_whitenbp, index=df.index)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "And plot them. I can see the black hole! Can you?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[[\"L1 whitened\", \"H1 whitened\"]][narrow_window].vgplot.line()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Before moving on, we have to do something about the date axis on that plot. It's useless. Let's change it to seconds before and after the origin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df.copy()\n",
    "df2.index = (df2.index - originutc).total_seconds()\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.index.name = \"seconds before or after origin\"\n",
    "df2[[\"L1 whitened\", \"H1 whitened\"]][narrow_window].vgplot.line(\n",
    "    value_name=\"whitened strain (units of noise stdev)\", var_name=\"source\", width=800, height=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's add a template (simulation) to overlay on the data. This looks daunting until you realize that it's the entirety of their Monte Carlo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the template (plus and cross) and parameters for the theoretical waveform\n",
    "f_template = h5py.File(\"LIGO/\" + event[\"fn_template\"], \"r\")\n",
    "\n",
    "# extract metadata from the template file:\n",
    "template_p, template_c = f_template[\"template\"][...]\n",
    "t_m1 = f_template[\"/meta\"].attrs[\"m1\"]\n",
    "t_m2 = f_template[\"/meta\"].attrs[\"m2\"]\n",
    "t_a1 = f_template[\"/meta\"].attrs[\"a1\"]\n",
    "t_a2 = f_template[\"/meta\"].attrs[\"a2\"]\n",
    "t_approx = f_template[\"/meta\"].attrs[\"approx\"]\n",
    "f_template.close()\n",
    "\n",
    "# the template extends to roughly 16s, zero-padded to the 32s data length. The merger will be roughly 16s in.\n",
    "template_offset = 16.0\n",
    "\n",
    "# whiten the templates:\n",
    "template_p_whiten = whiten(template_p, psd_H1)\n",
    "template_c_whiten = whiten(template_c, psd_H1)\n",
    "template_p_whitenbp = scipy.signal.filtfilt(bb, ab, template_p_whiten)\n",
    "template_c_whitenbp = scipy.signal.filtfilt(bb, ab, template_c_whiten)\n",
    "\n",
    "# constants:\n",
    "clight = 2.99792458e8                # m/s\n",
    "G = 6.67259e-11                      # m^3/kg/s^2 \n",
    "MSol = 1.989e30                      # kg\n",
    "\n",
    "# template parameters: masses in units of MSol:\n",
    "t_mtot = t_m1 + t_m2\n",
    "# final BH mass is typically 95% of the total initial mass:\n",
    "t_mfin = t_mtot*0.95\n",
    "# Final BH radius, in km:\n",
    "R_fin = 2*G*t_mfin*MSol/clight**2/1000.0\n",
    "\n",
    "# complex template:\n",
    "template = (template_p + template_c*10.j)\n",
    "ttime = time_H1 - time_H1[0] - template_offset\n",
    "\n",
    "# compute the instantaneous frequency of this chirp-like signal:\n",
    "tphase = numpy.unwrap(numpy.angle(template))\n",
    "fGW = numpy.gradient(tphase)*event[\"fs\"]/(2.0*numpy.pi)\n",
    "# fix discontinuities at the very end:\n",
    "# iffix = numpy.where(numpy.abs(numpy.gradient(fGW)) > 100.)[0]\n",
    "iffix = numpy.where(numpy.abs(template) < numpy.abs(template).max()*0.001)[0]\n",
    "fGW[iffix] = fGW[iffix[0] - 1]\n",
    "fGW[numpy.where(fGW < 1.0)] = fGW[iffix[0] - 1]\n",
    "\n",
    "# compute v/c:\n",
    "voverc = (G*t_mtot*MSol*numpy.pi*fGW/clight**3)**(1.0/3.0)\n",
    "\n",
    "# index where f_GW is in-band:\n",
    "f_inband = event[\"fband\"][0]\n",
    "iband = numpy.where(fGW > f_inband)[0][0]\n",
    "# index at the peak of the waveform:\n",
    "ipeak = numpy.argmax(numpy.abs(template))\n",
    "\n",
    "# number of cycles between inband and peak:\n",
    "Ncycles = (tphase[ipeak] - tphase[iband])/(2.0*numpy.pi)\n",
    "\n",
    "# -- To calculate the PSD of the data, choose an overlap and a window (common to all detectors)\n",
    "#   that minimizes \"spectral leakage\" https://en.wikipedia.org/wiki/Spectral_leakage\n",
    "NFFT = 4*event[\"fs\"]\n",
    "psd_window = numpy.blackman(NFFT)\n",
    "# and a 50% overlap:\n",
    "NOVL = NFFT/2\n",
    "\n",
    "# define the complex template, common to both detectors:\n",
    "template = (template_p + template_c*1.0j)\n",
    "# We will record the time where the data match the END of the template.\n",
    "etime = time_H1 + template_offset\n",
    "# the length and sampling rate of the template MUST match that of the data.\n",
    "datafreq = numpy.fft.fftfreq(template.size)*event[\"fs\"]\n",
    "\n",
    "# to remove effects at the beginning and end of the data stretch, window the data\n",
    "# https://en.wikipedia.org/wiki/Window_function#Tukey_window\n",
    "try:\n",
    "    # Tukey window preferred, but requires recent scipy version\n",
    "    dwindow = scipy.signal.tukey(template.size, alpha=1.0/8)\n",
    "except:\n",
    "    # Blackman window OK if Tukey is not available\n",
    "    dwindow = scipy.signal.blackman(template.size)\n",
    "\n",
    "# prepare the template fft.\n",
    "template_fft = numpy.fft.fft(template*dwindow) / event[\"fs\"]\n",
    "\n",
    "data = strain_L1.copy()\n",
    "\n",
    "data_psd, freqs = matplotlib.mlab.psd(data, Fs=event[\"fs\"], NFFT=NFFT, window=psd_window, noverlap=NOVL)\n",
    "\n",
    "# Take the Fourier Transform (FFT) of the data and the template (with dwindow)\n",
    "data_fft = numpy.fft.fft(data*dwindow) / event[\"fs\"]\n",
    "\n",
    "# -- Interpolate to get the PSD values at the needed frequencies\n",
    "power_vec = numpy.interp(numpy.abs(datafreq), freqs, data_psd)\n",
    "\n",
    "# -- Calculate the matched filter output in the time domain:\n",
    "# Multiply the Fourier Space template and data, and divide by the noise power in each frequency bin.\n",
    "# Taking the Inverse Fourier Transform (IFFT) of the filter output puts it back in the time domain,\n",
    "# so the result will be plotted as a function of time off-set between the template and the data:\n",
    "optimal = data_fft * template_fft.conjugate() / power_vec\n",
    "optimal_time = 2*numpy.fft.ifft(optimal)*event[\"fs\"]\n",
    "\n",
    "# -- Normalize the matched filter output:\n",
    "# Normalize the matched filter output so that we expect a value of 1 at times of just noise.\n",
    "# Then, the peak of the matched filter output will tell us the signal-to-noise ratio (SNR) of the signal.\n",
    "sigmasq = 1*(template_fft * template_fft.conjugate() / power_vec).sum() * numpy.abs(datafreq[1] - datafreq[0])\n",
    "sigma = numpy.sqrt(numpy.abs(sigmasq))\n",
    "SNR_complex = optimal_time/sigma\n",
    "\n",
    "# shift the SNR vector by the template length so that the peak is at the END of the template\n",
    "peaksample = int(data.size / 2)  # location of peak in the template\n",
    "SNR_complex = numpy.roll(SNR_complex, peaksample)\n",
    "SNR = abs(SNR_complex)\n",
    "\n",
    "# find the time and SNR value at maximum:\n",
    "indmax = numpy.argmax(SNR)\n",
    "timemax = time_H1[indmax]\n",
    "SNRmax = SNR[indmax]\n",
    "\n",
    "# Calculate the \"effective distance\" (see FINDCHIRP paper for definition)\n",
    "# d_eff = (8. / SNRmax)*D_thresh\n",
    "d_eff = sigma / SNRmax\n",
    "# -- Calculate optimal horizon distnace\n",
    "horizon = sigma / 8\n",
    "\n",
    "# Extract time offset and phase at peak\n",
    "phase = numpy.angle(SNR_complex[indmax])\n",
    "offset = (indmax-peaksample)\n",
    "\n",
    "# apply time offset, phase, and d_eff to whitened template, for plotting\n",
    "template_whitened = (template_p_whitenbp + template_c_whitenbp*1.0j) \n",
    "template_phaseshifted = numpy.real(template_whitened*numpy.exp(1j*phase))\n",
    "template_match = numpy.roll(template_phaseshifted,offset) / d_eff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Now add it to the data frame and plot. Perfect!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2[\"template\"] = pandas.Series(template_match, index=df2.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2[[\"L1 whitened\", \"H1 whitened\", \"template\"]][narrow_window].vgplot.line(\n",
    "    value_name=\"whitened strain (units of noise stdev)\", var_name=\"source\", width=1000, height=500)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
