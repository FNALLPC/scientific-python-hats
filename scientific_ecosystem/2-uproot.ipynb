{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# uproot\n",
    "\n",
    "ROOT data to Numpy arrays"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "There are several ways to get data from ROOT files into Numpy arrays.\n",
    "\n",
    "   * iteration in PyROOT (super slow!)\n",
    "   * ROOT's new `TTree::AsMatrix` (flat data, simple types)\n",
    "   * custom C++ function (defined through `ROOT.gInterpreter.Declare`)\n",
    "   * root_numpy (compiles against a ROOT version; can segfault with version mismatch)\n",
    "   * uproot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Unlike all of the above, **uproot** is a *reimplementation* of ROOT I/O that skips unnecessary steps between deserialization and array filling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "uproot uses Numpy vectorization for anything that scales with the number of events, Python for the complex business of navigating the file.\n",
    "\n",
    "For larger (fewer) baskets, there's less navigation and more vectorization.\n",
    "\n",
    "<table>\n",
    "  <tr style=\"background-color: white;\">\n",
    "    <td style=\"text-align: center; border-bottom: none; font-size: 18pt;\">Speedup relative to bare ROOT vs basket size</td>\n",
    "    <td style=\"text-align: center; border-bottom: none; font-size: 18pt;\">Speedup relative to root_numpy vs basket size</td>\n",
    "  </tr>\n",
    "  <tr style=\"background-color: white;\">\n",
    "    <td><img src=\"img/uproot_root-none-muon.png\"></td>\n",
    "    <td><img src=\"img/uproot_rootnumpy-none-muon.png\"></td>\n",
    "  </tr>\n",
    "</table>\n",
    "\n",
    "ROOT builds objects for the convenience of physics C++ code, but when dumping into arrays, we don't want that. That's why it can be a little faster than bare ROOT for large baskets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "uproot makes ROOT files, directories, and TTrees act like `dicts`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uproot\n",
    "\n",
    "nanoaod = uproot.open(\"~/NanoAOD-DYJetsToLL.root\")\n",
    "nanoaod.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = nanoaod[\"Events\"]\n",
    "tree.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Can we interpret that type? (If not interpretable, the third column is `None`. NanoAOD has no streamers/classes.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "tree.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Read one branch into an array at a time, or get a `dict`/`tuple`/`DataFrame` of arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree.array(\"MET_pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Numpy doesn't have a type for data whose size can vary per event (e.g. variable number of muons per event), so we use uproot's `JaggedArrays` for such branches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mupt = tree[\"Muon_pt\"].array()\n",
    "mueta = tree[\"Muon_eta\"].array()\n",
    "mupt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy, itertools, math, time\n",
    "\n",
    "starttime = time.time()\n",
    "\n",
    "pz = numpy.empty(len(mupt))\n",
    "i = 0\n",
    "for pts, etas in itertools.izip(mupt, mueta):     # you can do nested loops on these\n",
    "    for pt, eta in zip(pts, etas):                # as though they were lists within lists\n",
    "        pz[i] = pt * math.sinh(eta)\n",
    "        i += 1\n",
    "        break\n",
    "\n",
    "time.time() - starttime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "But actually each `JaggedArray` is an object with `content`, `starts`, and `stops` arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mupt.content)                              # the actual data (without event boundaries)\n",
    "print(mupt.starts)                               # where each event starts\n",
    "print(mupt.stops)                                # where each event stops\n",
    "mupt.starts.base is mupt.stops.base              # (starts and stops are just views of the same offsets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "starttime = time.time()\n",
    "\n",
    "hasamuon = mupt.stops - mupt.starts > 0           # remember this trick from numpy.ipynb?\n",
    "firsts = mupt.starts[hasamuon]\n",
    "pz = mupt.content[firsts] * numpy.sinh(mueta.content[firsts])\n",
    "\n",
    "time.time() - starttime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "uproot has no implicit caching or parallel processing. It must be explicitly requested."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cache = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "starttime = time.time()\n",
    "tree.arrays(\"Jet_*\", cache=cache)    # first time: reads file; second time: gets from dict\n",
    "time.time() - starttime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "A `dict` is not really a cache because it never evicts old data to make space. `MemoryCache` is a drop-in replacement that does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(uproot.cache.MemoryCache)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "You control your own memory use, either by making a `MemoryCache` the right size or by explicitly clearing `dicts` when you need to."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Parallel processing uses Python's Executor model (similar to TBB, but much less developed)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "executor = ThreadPoolExecutor(4)                   # number of cores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "starttime = time.time()\n",
    "again = tree.arrays(\"Jet_*\", executor=executor)    # that's happening in parallel\n",
    "time.time() - starttime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "With a lot of computational work (LZMA decompression in the case below), this can make a difference.\n",
    "\n",
    "<img src=\"img/uproot_scaling.png\" style=\"display: block; margin-left: auto; margin-right: auto\">\n",
    "\n",
    "(That is, parallel processing isn't ruined by Python's interpreter lock: numerical libraries such as Numpy and LZMA escape this constraint to actually run in parallel.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "For large files or many-file datasets, you'll want to iterate: not over events, but arrays (chunks of events)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for arrays in tree.iterate(\"Jet_*\"):\n",
    "    print(\"batch of {} arrays, {} MB\".format(len(arrays), sum(x.nbytes / 1024.0**2 for x in arrays.values())))\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The default chunk size is the ROOT \"cluster size,\" but this is highly configurable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Nearly the same syntax for multiple files (like TChain)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "for arrays in uproot.iterate(\"~/NanoAOD-*.root\", \"Events\", \"Jet_*\"):\n",
    "    print(\"batch of {} arrays, {} MB\".format(len(arrays), sum(x.nbytes / 1024.0**2 for x in arrays.values())))\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Appliction: dropping data into machine learning libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Define a 2 hidden layer neural network in PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class SimpleNN(torch.nn.Module):\n",
    "    def __init__(self, input_dim, hidden1_dim, hidden2_dim, output_dim):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.layer1 = torch.nn.Linear(input_dim, hidden1_dim)\n",
    "        self.relu1 = torch.nn.ReLU()\n",
    "        self.layer2 = torch.nn.Linear(hidden1_dim, hidden2_dim)\n",
    "        self.relu2 = torch.nn.ReLU()\n",
    "        self.layer3 = torch.nn.Linear(hidden2_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layer3(self.relu2(self.layer2(self.relu1(self.layer1(x)))))\n",
    "\n",
    "# 25 input parameters, 20 node hidden layer, 10 node hidden layer, 1 output\n",
    "simplenn = SimpleNN(25, 20, 10, 1)\n",
    "\n",
    "criterion = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(simplenn.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The 25 input parameters are jet attributes other than the btag.\n",
    "\n",
    "The 1 output is the supervised learning target: Jet_btagCMVA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "jetarrays = tree.arrays(\"Jet_*\")\n",
    "\n",
    "inputs = numpy.vstack(jetarrays[n] for n in sorted(jetarrays) if not n.startswith(\"Jet_btag\")).T.astype(\"float32\")\n",
    "expected_output = numpy.array(jetarrays[\"Jet_btagCMVA\"]).reshape(-1, 1)\n",
    "\n",
    "inputs.shape, expected_output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "PyTorch, like all other Pythonic ML libraries, has methods to get batches of data from Numpy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "inputs = torch.autograd.Variable(torch.from_numpy(inputs))\n",
    "expected_output = torch.autograd.Variable(torch.from_numpy(expected_output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "And now we use PyTorch; it doesn't matter where the data came from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "optimizer.zero_grad()\n",
    "computed_output = simplenn.forward(inputs)\n",
    "loss = criterion(computed_output, expected_output)\n",
    "loss.backward()\n",
    "optimizer.step()\n",
    "print(loss)                # I have _in no way_ demonstrated that we have a good b-tag training. Just sayin'."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
